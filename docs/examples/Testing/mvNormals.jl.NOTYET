# ### Assessing estimates of mean energy and free energy 

# We want to understand how well our sampler approximates ``\mathbb{E}^{(\beta)}[V]`` and ``\mathcal{F}(\beta)``. To do this, we first carry out the tuning procedure. The free energy estimates correspond to `c` under the current tuning strategy. For the mean energy, we run the sampler again, and using the resulting `ParallelRunResults` object, we request an estimation that also returns an approximation of the asymptotic Monte Carlo variance
# ```@example tg; continued = true
# tune!(samplers, verbose=true)
# restune = parallel_run!(samplers, ntours=512*Threads.nthreads());
# means, vars = estimate(restune, ns.np.V)
# ```
# Let us visually inspect the results. We use the estimated variance to produce an approximate 95% confidence interval around each point estimate.
# ```@example tg; continued = false
# # energy
# p1 = plot(F',0.,1., label="Theory", title="Mean energy")
# plot!(
#     p1, ns.np.betas, means, label="Estimate", seriestype=:scatter,
#     yerror = 1.96sqrt.(vars/restune.ntours)
# )

# # free-energy
# p2 = plot(
#     b -> F(b)-F(0.), 0., 1., label="Theory", legend_position = :bottomright,
#     title = "Free energy"
# )
# plot!(p2, ns.np.betas, samplers[1].np.c, label="Estimate", seriestype = :scatter)

# plot(p1,p2,size=(800,400))
# ```
# The first figure shows a high level of agreement between the estimated mean energies and the theoretical values. The second plot shows more disagreement betweeen the theoretical and approximated values of the free energy. This means that the discrepancy could be corrected by having a finer grid.


# ### Uniformity of the distribution over levels

# Under the mean-energy tuning strategy, we expect a uniform distribution over the levels. We can check this by inspecting the trace of the previous section. Also, we can do the same for the case where we set `c` to the exact value of the free energy
# ```@example tg; continued = true
# copyto!(ns.np.c, F.(ns.np.betas))
# resexact = parallel_run!(samplers, ntours=512*Threads.nthreads());
# full_postprocessing!(resexact) # computes the :visits field and others
# ```

# !!! note "`np` is shared"
#     The `np` field is shared across `samplers`, and `ns=samplers[1]`. Thus, by changing `ns.np.c` we are effectively changing the setting for all the samplers.

# Let us visually inspect the results
# ```@example tg; continued = false
# xs = repeat(1:length(ns.np.c), 2)
# gs = repeat(["Exact c", "Tuned c"], inner=length(ns.np.c))
# ys = vcat(vec(sum(resexact.visits, dims=1)),vec(sum(restune.visits, dims=1)))
# groupedbar(
#     xs,ys,group=gs, legend_position=:topleft,
#     title="Number of visits to every level"
# )
# ```
# Setting `c` to its theoretical value under the mean-energy strategy indeed gives a very uniform distribution over levels. The case where `c` is tuned, on the other, has a slight bias towards the levels closer to the target measure.

# ### Scaling of the maximal tour length and duration

# Here the samplers are run multiple rounds, using an exponentially increasing number of tours. For each round, we compute the maximal tour length and duration in seconds. Each round is repeated `nreps` times to assess the variability of these measurements.
# ```@example tg; continued = true
# function max_scaling(samplers, nrounds, nreps)
#     ntours  = Threads.nthreads()*round.(Int, 2 .^(0:(nrounds-1)))
#     msteps  = Matrix{Int}(undef, nreps, nrounds)
#     mtimes  = Matrix{Float64}(undef, nreps, nrounds)
#     for rep in 1:nreps
#         for r in 1:nrounds
#             res = parallel_run!(samplers, ntours=ntours[r])
#             tour_durations!(res) # populates only the :nsteps and :times fields
#             msteps[rep,r] = maximum(res.nsteps)
#             mtimes[rep,r] = maximum(res.times)
#         end
#     end
#     return ntours, msteps, mtimes
# end
# ```
# We must run the function first to avoid counting compilation times
# ```@example tg; continued = true
# ntours, msteps, mtimes = max_scaling(samplers, 2, 2)
# ```
# Now we may proceed with the experiment
# ```@example tg; continued = false
# ntours, msteps, mtimes = max_scaling(samplers, 10, 50)
# p1 = plot()
# p2 = plot()
# for (r,m) in enumerate(eachrow(msteps))
#     plot!(p1, ntours, m, xaxis=:log, linealpha = 0.2, label="", linecolor = :blue)
#     plot!(p2, ntours, mtimes[r,:], xaxis=:log, linealpha = 0.2, label="", linecolor = :blue)
# end
# plot!(
#     p1, ntours, vec(sum(msteps,dims=1))/size(msteps,1), xaxis=:log,
#     linewidth = 2, label="Average", linecolor = :blue, legend_position=:topleft,
#     xlabel="Number of tours", ylabel="Maximum number of steps across tours"
# )
# plot!(
#     p2, ntours, vec(sum(mtimes,dims=1))/size(mtimes,1), xaxis=:log,
#     linewidth = 2, label="Average", linecolor = :blue, legend_position=:topleft,
#     xlabel="Number of tours", ylabel="Time to complete longest tour (s)"
# )
# using Plots.PlotMeasures
# plot(p1,p2,size=(1000,450), margin = 4mm)
# ```


# ### Visual inspection of samples

# Here we compare the contours of the pdf of the annealed distributions versus the samples obtained at each of the levels when NRST is run using the tuned `c`. As we know from the theory, ergodicity holds for any (reasonable) `c`, so we should expect to see agreement between contours and samples.

# ```@example tg
# function draw_contour!(p,b,xrange)
#     dist = MultivariateNormal(mu(b), sbsq(b)*I(d))
#     f(x1,x2) = pdf(dist,[x1,x2])
#     Z = f.(xrange, xrange')
#     contour!(p,xrange,xrange,Z,levels=lvls,aspect_ratio = 1)
# end

# function draw_points!(p,i;x...)
#     M = reduce(hcat,restune.xarray[i])
#     scatter!(p, M[1,:], M[2,:];x...)
# end

# if d == 2
#     # plot!
#     xmax = 4*s0
#     xrange = -xmax:0.1:xmax
#     xlim = extrema(xrange)
#     minloglev = logpdf(MultivariateNormal(mu(0.), sbsq(0.)*I(d)), 3*s0*ones(2))
#     maxloglev = logpdf(MultivariateNormal(mu(1.), sbsq(1.)*I(d)), mu(1.))
#     lvls = exp.(range(minloglev,maxloglev,length=10))
#     anim = @animate for (i,b) in enumerate(ns.np.betas)
#         p = plot()
#         draw_contour!(p,b,xrange)
#         draw_points!(
#             p,i;xlim=xlim,ylim=xlim,
#             markeralpha=0.3,markerstrokewidth=0,
#             legend_position=:bottomleft,label="Î²=``(round(b,digits=2))"
#         )
#         plot(p)
#     end
#     gif(anim, fps = 2)
# end
# ```
# We see that the samples correctly describe the pdf of the corresponding distributions.

